# Linear Algebra Foundations for Machine Learning

This module contains from-scratch implementations of core linear algebra concepts used in machine learning.

---

## üéØ Purpose

- Build mathematical intuition
- Implement core operations manually
- Connect linear algebra to ML systems

---

## üìö Implemented Components

---

### 1Ô∏è‚É£ Dot Product

**Mathematical Definition**

For two vectors:

u = (u‚ÇÅ, u‚ÇÇ, ..., u‚Çô)  
v = (v‚ÇÅ, v‚ÇÇ, ..., v‚Çô)

The dot product is:

u ¬∑ v = Œ£ (u·µ¢ v·µ¢)

**Why It Matters**

- Measures similarity between vectors
- Core operation in matrix multiplication
- Used in neural network computations

**Implementation**

`dot(matrix1, matrix2, r, c)`

---

### 2Ô∏è‚É£ Matrix Multiplication

**Mathematical Definition**

If:

- Matrix A has shape (m √ó n)
- Matrix B has shape (n √ó p)

Then:

- The product C = A √ó B has shape (m √ó p)

Each element is computed as:

C[i][j] = dot(row_i_of_A, column_j_of_B)

**Why It Matters**

- Core of neural network layers
- Used in linear transformations
- Foundation of most ML algorithms

**Implementation**

`dot_matrix(matrix1, matrix2)`

---

## üß™ Validation

All implementations are verified against NumPy:

```python
assert np.allclose(dot_matrix(A, B), A @ B)
### 3Ô∏è‚É£ Matrix Properties (Planned / In Progress)
- Transpose
- Determinant
- Inverse
- Rank
- Trace

### 4Ô∏è‚É£ Decompositions (Planned)
- LU Decomposition
- QR Decomposition
- Eigenvalues & Eigenvectors
- Singular Value Decomposition (SVD)
